{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0a6951a",
   "metadata": {},
   "source": [
    "# MNIST Handwritten Digits Classification (PyTorch CNN)\n",
    "\n",
    "This notebook trains a Convolutional Neural Network (CNN) on the MNIST dataset to classify handwritten digits. It includes model architecture, training loop, evaluation, and visualization of sample predictions. The target is **>95% test accuracy**.\n",
    "\n",
    "Run the cells in order. If you have a GPU available, PyTorch will use it automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80a1d39",
   "metadata": {},
   "source": [
    "## 1. Imports and device setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc1ce33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "print('PyTorch version:', torch.__version__)\n",
    "\n",
    "# Reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37cc99c",
   "metadata": {},
   "source": [
    "## 2. Prepare MNIST dataset and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33adbff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Transform: normalize to mean=0.1307, std=0.3081 (standard for MNIST)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Download datasets (will store in ./data)\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "test_dataset  = datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "print('Train samples:', len(train_dataset))\n",
    "print('Test samples:', len(test_dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c49be90",
   "metadata": {},
   "source": [
    "## 3. Define the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6e11f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        # conv layers\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)  # 28x28 -> 28x28\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1) # 28x28 -> 28x28\n",
    "        self.pool = nn.MaxPool2d(2, 2)                           # 28x28 -> 14x14 after pool\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        # fully connected\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)               # shape: [batch, 64, 14, 14] after second conv+pool\n",
    "        x = self.dropout1(x)\n",
    "        x = x.view(x.size(0), -1)      # flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleCNN().to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d2b380",
   "metadata": {},
   "source": [
    "## 4. Training and evaluation helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0578632d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_one_epoch(model, device, train_loader, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * data.size(0)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct += preds.eq(target).sum().item()\n",
    "        total += data.size(0)\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    print(f\"Train Epoch: {epoch} \\tLoss: {epoch_loss:.4f} \\tAccuracy: {epoch_acc:.4f}\")\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def evaluate(model, device, data_loader, criterion=None):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            outputs = model(data)\n",
    "            if criterion is not None:\n",
    "                running_loss += criterion(outputs, target).item() * data.size(0)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_targets.append(target.cpu().numpy())\n",
    "            correct += preds.eq(target).sum().item()\n",
    "            total += data.size(0)\n",
    "    loss = (running_loss / total) if (criterion is not None) else None\n",
    "    acc = correct / total\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    return loss, acc, all_preds, all_targets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a082b912",
   "metadata": {},
   "source": [
    "## 5. Train the model\n",
    "\n",
    "We'll train for up to 10 epochs with early stopping by monitoring test accuracy. Use Adam optimizer and CrossEntropyLoss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc436ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training setup\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 10\n",
    "\n",
    "best_acc = 0.0\n",
    "patience = 3\n",
    "epochs_no_improve = 0\n",
    "\n",
    "train_history = {'loss': [], 'acc': []}\n",
    "test_history = {'loss': [], 'acc': []}\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    train_loss, train_acc = train_one_epoch(model, device, train_loader, optimizer, criterion, epoch)\n",
    "    test_loss, test_acc, _, _ = evaluate(model, device, test_loader, criterion)\n",
    "    print(f\"Test  Epoch: {epoch} \\tLoss: {test_loss:.4f} \\tAccuracy: {test_acc:.4f}\\n\")\n",
    "    \n",
    "    train_history['loss'].append(train_loss)\n",
    "    train_history['acc'].append(train_acc)\n",
    "    test_history['loss'].append(test_loss)\n",
    "    test_history['acc'].append(test_acc)\n",
    "    \n",
    "    # Early stopping / save best\n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "        epochs_no_improve = 0\n",
    "        # Save best model state dict\n",
    "        torch.save(model.state_dict(), 'best_mnist_cnn.pth')\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping after {epoch} epochs. Best test acc: {best_acc:.4f}\")\n",
    "            break\n",
    "\n",
    "print('Training finished. Best test accuracy:', best_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd68ba7",
   "metadata": {},
   "source": [
    "## 6. Load best model and evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab295c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load best model (if saved)\n",
    "best_model = SimpleCNN().to(device)\n",
    "try:\n",
    "    best_model.load_state_dict(torch.load('best_mnist_cnn.pth', map_location=device))\n",
    "    print('Loaded best model weights.')\n",
    "except Exception as e:\n",
    "    print('Could not load saved model, using current model. Error:', e)\n",
    "\n",
    "test_loss, test_acc, test_preds, test_targets = evaluate(best_model, device, test_loader, criterion)\n",
    "print(f\"Final Test Accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87dd937",
   "metadata": {},
   "source": [
    "## 7. Visualize predictions on 5 random test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23512c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Pick 5 random samples from test set and show predictions\n",
    "import matplotlib.pyplot as plt\n",
    "idxs = np.random.choice(len(test_dataset), size=5, replace=False)\n",
    "fig, axes = plt.subplots(1, 5, figsize=(12,3))\n",
    "for ax, idx in zip(axes, idxs):\n",
    "    img, label = test_dataset[idx]\n",
    "    # img is normalized tensor; unnormalize for display\n",
    "    img_disp = img.squeeze().numpy() * 0.3081 + 0.1307\n",
    "    ax.imshow(img_disp, cmap='gray')\n",
    "    ax.axis('off')\n",
    "    # model prediction\n",
    "    best_model.eval()\n",
    "    with torch.no_grad():\n",
    "        input_tensor = img.unsqueeze(0).to(device)\n",
    "        out = best_model(input_tensor)\n",
    "        pred = out.argmax(dim=1).item()\n",
    "    ax.set_title(f\"true: {label}\\npred: {pred}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e320fe4",
   "metadata": {},
   "source": [
    "## 8. Plot training curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c86f4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(train_history['loss'], label='train loss')\n",
    "plt.plot(test_history['loss'], label='test loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss curves')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(train_history['acc'], label='train acc')\n",
    "plt.plot(test_history['acc'], label='test acc')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Accuracy curves')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f657f292",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "**Notes:**\n",
    "- Achieving >95% test accuracy on MNIST is straightforward with this model; if accuracy is low, increase `num_epochs`, tweak model size, or add learning rate scheduling.\n",
    "- The notebook saves the best model to `best_mnist_cnn.pth`.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
